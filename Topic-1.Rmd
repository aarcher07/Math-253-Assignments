# Topic 1 Exercises

## Exercise 1 in ISL  2.4

a. More flexible -- There is more data present. Using a function that more flexible function will capture attempt to capture as much variance in data points as possible so it will now the bias. Also adding a data will not significantly affect f since n is extremely large. So both


b. Less flexible. I would use a less flexible method. Otherwise a more flexible method would overfit the data meaning that it would capture too much noise.

c. More flexible.  More flexible models capture non linear relationships better.

d. Both would be equally as bad. Either would reduce Bias$(\hat{f}(x_0))$ or Var$(\hat{f}(x_0))$ but not reduce Var$(\epsilon)$.


## 2.4.2

2a) This is a regression problem. We are interested in inference. $n=500$ and $p=4$.

2b) This is a classification problem and we are most interested in prediction. $n = 20$ and $p= 14$.

2c) This is a regression problem and we are most interested in prediction. 


## 2.4.3

## (a)  Graphs of Bias-Variance
```{r}
bias=function(x){1/log(log(x))}
varf = function(x){(x-6)^6/100}
training_error=function(x){-(x-6)^5/50+20}
testing_error = function(x){varf(x)+bias(x)}
bayes=function(x){rep(20,length(x))}

xx=seq(2760,10000)/1000
plot(xx,xlab='Flexibility', ylab='Value',varf(xx),type='l',col='red')
lines(xx,bias(xx),type='l')
lines(xx,training_error(xx), col='blue')
lines(xx,testing_error(xx), col='orange')
lines(xx,bayes(xx), col='pink')

legend(7,40,c('Variance','Bias','Training error','Testing error','Bayes'),lty = 1,col=c('red','black','blue','orange','pink'))

```


## (b) Description of Curves

As we increase the flexibility, the bias decreases because $\hat{f}$ starts approximates the real world data and its non-linearities more closely.

As we increase the flexibility, the variance increases. This is because, if a new data point is added, $\hat{f}$ attempt to accurate the new variability of data. This may change $\hat{f}$ completely.

The training error decreases as we increase flexibility. Increase flexibility of model allows the model to approximate the training data more closely and, invariably, capturing noise while doing so. 

Test MSE is the sum of the Var$(\epsilon)$, bias and Var$(\hat{f})$. While bias deceases to zero as the flexibility increases, Var$(\hat{f})$ increases. So MSE decreases to minimum and increases again.

Bayes error rate is the minimum error in classification problem. It stays constant as we vary flexibility. 


## 2.4.6

Parametric methods involve making assumptions about the functional form of $f$. Whereas non-parametric methods do not. Non parametric methods avoid deducing the incorrect functional form of f whereas parametric methods can. As a result of its flexible, non parametric methods can take on a variety of functional forms. However, non-parametric methods require a large amount of data to correctly determine $f$.


## 2.4.7
## (a) Compute euclidean distance between origin
Observation (1) - 3
Observation (2) - 2
Observation (3) - 10
Observation (4) - 5
Observation (5) - 2
Observation (6) - 3

## (b) Prediction for $K=1$
5 is its closest neighbour so I would guess that the test point is red.
## (c) Prediction for $K=3$
2 out of the 3 closest neighbours are red so I would guess that the test point is red.
## (d) Best for value for K

I would expect the best K to be small since small K capture high variablity in data.

## 2.4.8 (a)
```{r}
college= read.csv("/Users/aarcher/Desktop/Maching Learning/College.csv")
```

## 2.4.8 (b)

```{r}
college=college[,-1]
fix(college)
```

## 2.4.8 (c) (i) 

```{r}
summary(college)
```

## 2.4.8 (c) (ii) 
```{r}
pairs(college[,1:10])
```

## 2.4.8 (c) (iii) 
```{r}
plot(college$Private,college$Outstate)
```

## 2.4.8 (c) (iv) 
```{r}
Elite=rep("No",nrow(college))
Elite[college$Top10perc >50]="Yes"
Elite=as.factor(Elite)
college=data.frame(college ,Elite)
```

```{r}
summary(college)
plot(college$Elite,college$Outstate)
```

## 2.4.8 (c) (v) 
```{r}
par(mfrow=c(2,2))
hist(college$Top25perc,col=2,breaks=20)
hist(college$Top10perc,col=2,breaks=15)
hist(college$Room.Board,col=2,breaks=10)
hist(college$Outstate,col=2,breaks=5)
```

## 2.4.9 (a)  
```{r}
auto_file_name= "/Users/aarcher/Desktop/Maching Learning/Math-253-Assignments/Auto.csv"
Auto<-read.csv(auto_file_name,header=T,na.strings ="?")
Auto=na.omit(Auto)
fix(Auto)
names(Auto)
```

The quantitative variables are mpg, displacement, hoursepower, weight and acceleration. Cylinders, year and origin in the absolute sense is quantitative variable. The qualitative variable is name. Although cylinders, year and origin are represented as numerics, we can treat them as qualitative variables. 

## 2.4.9 (b)

```{r}
range(Auto$mpg)
range(Auto$cylinders)
range(Auto$displacement)
range(Auto$horsepower)
range(Auto$weight)
range(Auto$acceleration)
range(Auto$year)
range(Auto$origin)
```
Mpg: 9.0 - 46.6
Cylinders: 3-8
Displacement: 68 - 455
Horsepower: 46 - 230
Weight: 1613 - 5140
Acceleration: 8.0 - 24.8
Year: 70 - 82
Origin: 1-3

## 2.4.9 (c)
```{r}
mean(Auto$mpg)
mean(Auto$cylinders)
mean(Auto$displacement)
mean(Auto$horsepower)
mean(Auto$weight)
mean(Auto$acceleration)
mean(Auto$year)
mean(Auto$origin)

sd(Auto$mpg)
sd(Auto$cylinders)
sd(Auto$displacement)
sd(Auto$horsepower)
sd(Auto$weight)
sd(Auto$acceleration)
sd(Auto$year)
sd(Auto$origin)
```

Mpg: mean - 23.4, sd - 7.8
Cylinders: mean - 5.47, sd - 1.70
Displacement: mean - 194.4, sd - 104.6
Horsepower: mean - 104.5, sd - 38.5
Weight: mean - 2977.6, sd - 849.4
Acceleration: mean: 15.54, sd - 2.75
Year: mean: 75.98, sd - 3.68
Origin: 1.57, sd - 0.81

## 2.4.9 (d)

```{r}
Auto = Auto[c(-10,-85),]

range(Auto$mpg)
range(Auto$cylinders)
range(Auto$displacement)
range(Auto$horsepower)
range(Auto$weight)
range(Auto$acceleration)
range(Auto$year)
range(Auto$origin)

mean(Auto$mpg)
mean(Auto$cylinders)
mean(Auto$displacement)
mean(Auto$horsepower)
mean(Auto$weight)
mean(Auto$acceleration)
mean(Auto$year)
mean(Auto$origin)

sd(Auto$mpg)
sd(Auto$cylinders)
sd(Auto$displacement)
sd(Auto$horsepower)
sd(Auto$weight)
sd(Auto$acceleration)
sd(Auto$year)
sd(Auto$origin)
```



The range of all the variables remained the same. The mean and standard deviation change for all the variables.



## 2.4.9 (e)

```{r}
Auto$cylinders=as.factor(Auto$cylinders)
Auto$origin= as.factor(Auto$origin)
Auto$year= as.factor(Auto$year)
```

```{r}
par(mfrow=c(2,2))
plot(Auto$cylinders,Auto$mpg)
plot(Auto$year,Auto$mpg)
plot(Auto$origin,Auto$mpg)
```


The mpg as function of cylinder follows parabola. On average, a cylinder of 4 has the highest mpg. 
On average, higher mpg are associated with higher years.
On average,  higher mpg are associated with origin increasing order.

```{r}
par(mfrow=c(3,3))
plot(Auto$horsepower,Auto$mpg)
plot(Auto$weight,Auto$horsepower)
plot(Auto$weight,Auto$mpg)
plot(Auto$displacement,Auto$mpg)
plot(Auto$displacement,Auto$horsepower)

```


Mpg and horsepower are negatively correlated. 
Weight and horsepower are positively correlated.
Weight and mpg are negatively correlated.
Displacement and mpg are negatively correlated.
Displacement and horsepower are positively correlated.

## 2.4.9 (f)

Yes, other variables could be used to predict mpg. Looking at the block and scatter plot of all the variables, you can see there is a strong between mpg and the variables.


